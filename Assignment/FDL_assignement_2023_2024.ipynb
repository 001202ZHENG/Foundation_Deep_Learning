{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8ZAjw4khuHc"
      },
      "source": [
        "<table>\n",
        "<td height=\"150px\">\n",
        "<img src='https://stergioc.github.io/assets/img/logos.png' />\n",
        "</td>\n",
        "</table>\n",
        "\n",
        "#FDL DSBA 2023-2024 Individual Assignment\n",
        "### **Deadline:** 03/01/2024 @ 23:59\n",
        "This is an individual assignement. You will be graded in 100 points while two different optional questions will be possible for you to have 20 extra points.\n",
        "\n",
        "Please fill in the blanks in between the `****START CODE****` and `****END CODE****` lines in the cells, and answer to the questions in Markdown in the first section where you see `ANSWER HERE`.\n",
        "\n",
        "In order to submit, please rename the file to `FDL_Assignment_<first_name>_<last_name>.ipynb` and upload your solution to Edunao after zipping it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Epe_MvwMiW1v"
      },
      "source": [
        "## Section 1. Getting started **[10 pts]**\n",
        "Answer these theoretical questions in the `ANSWER HERE` cells."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbDRtgEuigTi"
      },
      "source": [
        "**1.1**. What is a metric? What is a loss? What is the difference between both, and how are they used in the training process?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vB7wKGxoixau"
      },
      "source": [
        "`ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-aRODgXi4TQ"
      },
      "source": [
        "1.2. Briefly explain the concept of gradient descent, and how it is used in the training process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9Qzg428jXGR"
      },
      "source": [
        "`ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFPJkEuTja8k"
      },
      "source": [
        "1.3. Explain the bias-variance tradeoff problem, and how it is linked with the concept of overfitting and underfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDJ12xIhjtJk"
      },
      "source": [
        "`ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_pi8mdvjyrx"
      },
      "source": [
        "1.4. What is an activation function, and why is it necessary in order to stack multiple layers?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHbGk8BDktE4"
      },
      "source": [
        "`ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvOGUm_Am0xQ"
      },
      "source": [
        "1.5. Briefly explain the CNN architecture, and why it is more adapted to images than standard MLP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdJfVAHhnava"
      },
      "source": [
        "`ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TS5LMbLg8_h"
      },
      "source": [
        "1.6. What is the difference between deep learning and classical machine learning? What are the main advantages of deep learning over more classical techniques?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "165eMvqKhll2"
      },
      "source": [
        "`ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0uEKaobjm9g"
      },
      "source": [
        "1.7. Discuss the ethical considerations and potential biases that may arise during a training of a deep learning model. How is it possible to take this into account?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7RvE4oqlgsS"
      },
      "source": [
        "`ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCf7cqT5neRw"
      },
      "source": [
        "1.8. Discuss the ethical implications of deploying deep learning models in critical processes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4LzOAuhou49"
      },
      "source": [
        "`ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-xTgUMt4XGA"
      },
      "source": [
        "1.9. List a couple of methods that you can use to help interpret the classifications of your neural network, and provide a brief explanation of how they work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DACKnypv4clB"
      },
      "source": [
        "`ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAVQPrpMvjYb"
      },
      "source": [
        "## Section 2. Training a CNN **[40 pts]**\n",
        "In this section, you will train a CNN on a dataset of [histopathology patches](https://en.wikipedia.org/wiki/Histopathology). This data corresponds to digitized microscopic analysis of tumor tissue, which has been divided into patches. The objective is to classify the patches into the ones containing tumor tissue, and ones not containing any tumor tissue. We will use the [PCAM dataset](https://github.com/basveeling/pcam) which consists of 96x96 pixel patches. We will only use the validation set (which contains 32768 patches and which should take about 0.8 GB of storage) in order to make the training faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NpZtqT5Fdoh"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.datasets.utils import download_file_from_google_drive, _decompress"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_5CIVY_B4tY"
      },
      "source": [
        "2.1. Download the dataset which is stored in a `.h5` file.\n",
        "The images can be download from [here](https://drive.google.com/uc?export=download&id=1hgshYGWK8V-eGRy8LToWJJgDU_rXWVJ3), and the labels from [here](https://drive.google.com/uc?export=download&id=1bH8ZRbhSVAhScTS0p9-ZzGnX91cHT3uO). Please then unzip the files and write the paths below. **[1 pt]**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUz7fdZRjlhp"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets.utils import download_file_from_google_drive, _decompress\n",
        "\n",
        "# You can run the following cell to download the files on colab\n",
        "base_folder = \"./\"\n",
        "archive_name = \"camelyonpatch_level_2_split_valid_x.h5.gz\"\n",
        "download_file_from_google_drive(\"1hgshYGWK8V-eGRy8LToWJJgDU_rXWVJ3\", base_folder, filename=archive_name, md5=\"d5b63470df7cfa627aeec8b9dc0c066e\")\n",
        "_decompress(base_folder + archive_name)\n",
        "\n",
        "archive_name = \"camelyonpatch_level_2_split_valid_y.h5.gz\"\n",
        "download_file_from_google_drive(\"1bH8ZRbhSVAhScTS0p9-ZzGnX91cHT3uO\", base_folder, filename=archive_name, md5=\"2b85f58b927af9964a4c15b8f7e8f179\")\n",
        "_decompress(base_folder + archive_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdOEHuVbnXFU"
      },
      "outputs": [],
      "source": [
        "# ****START CODE****\n",
        "IMAGES_PATH =\n",
        "LABELS_PATH =\n",
        "# ****END CODE****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8dIDByrHC8U"
      },
      "outputs": [],
      "source": [
        "images = np.array(h5py.File(IMAGES_PATH)['x'])\n",
        "labels = np.array([y.item() for y in h5py.File(LABELS_PATH)['y']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDcpDHPfIN1M"
      },
      "source": [
        "2.2. Now that we have the data, we will want to split it into a training and a validation set. For this, we will write a function which takes in as input the size of the dataset, and which will return the indices of the training set and the indices of the validation set. **[1 pt]**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZejRMe3nIjA8"
      },
      "outputs": [],
      "source": [
        "random.seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taCCj9BAIMtC"
      },
      "outputs": [],
      "source": [
        "def get_split_indices(dataset_length, train_ratio=0.7):\n",
        "    \"\"\"\n",
        "    Function which splits the data into tranining and validation sets.\n",
        "    arguments:\n",
        "        dataset_length [int]: number of elements in the dataset\n",
        "        train_ratio [float]: ratio of the dataset in the training set\n",
        "    returns:\n",
        "        train_indices [list]: list of indices in the training set (of size dataset_length*train_ratio)\n",
        "        val_indices [list]: list of indices in the validation set (of size dataset_length*(1-train_ratio))\n",
        "    \"\"\"\n",
        "    # ****START CODE****\n",
        "\n",
        "    # ****END CODE****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0K4d2UmJg1h"
      },
      "outputs": [],
      "source": [
        "train_indices, val_indices = get_split_indices(len(labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_T1sbPlFLQ9"
      },
      "source": [
        "2.3. Write the dataset classes. Feel free to add any type of data augmentation that you like. Please note that pytorch has an implemented PCAM dataset class, but we ask you to code these using from scratch. **[2 pt]**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTeBIythFKzR"
      },
      "outputs": [],
      "source": [
        "class PCAMDataset(Dataset):\n",
        "    def __init__(self, data, labels, train):\n",
        "        \"\"\"\n",
        "        Dataset class for the PCAM dataset.\n",
        "        arguments:\n",
        "            data [numpy.array]: all RGB 96-96 images\n",
        "            labels [numpy.array]: corresponding labels\n",
        "            train [bool]: whether the dataset is training or validation\n",
        "        \"\"\"\n",
        "        super(PCAMDataset, self).__init__()\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "        self.train = train\n",
        "\n",
        "        if self.train:\n",
        "            # ****START CODE****\n",
        "            self.augmentation = transforms.Compose([])\n",
        "            # ****END CODE****\n",
        "\n",
        "    def __len__(self):\n",
        "        # ****START CODE****\n",
        "\n",
        "        # ****END CODE****\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # ****START CODE****\n",
        "\n",
        "        # ****END CODE****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lfGMUmkKK-M"
      },
      "outputs": [],
      "source": [
        "# ****START CODE****\n",
        "BATCH_SIZE =\n",
        "# ****END CODE****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M139A5_qJt9N"
      },
      "outputs": [],
      "source": [
        "train_dataset = PCAMDataset(images[train_indices], labels[train_indices], train=True)\n",
        "val_dataset = PCAMDataset(images[val_indices], labels[val_indices], train=False)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0iDIwc8hWe1"
      },
      "source": [
        "2.4 Display a random sample of images that have a label of 0 (not containing\n",
        "any tumor tissue) and 1 (containing tumor tissue). **[2 pt]**\n",
        "\n",
        "Can you identify the features in a particular image which cause it to be classified as having tumor tissue or not?\n",
        "\n",
        "(Extra: See if you can display a random sample of images without looking at the label, and then try to classify it as containing tumor tissue or not - then check your answer afterwards)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t65N8O14jjyL"
      },
      "source": [
        "2.5. Plot the distribution of class labels in the training and validation datasets, to see how well the classes are balanced. **[1 pt]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Unl4FK49WS__"
      },
      "source": [
        "`ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQIOOG6bKtez"
      },
      "source": [
        "2.6. Write your model architecture, you can be creative here! Here is a (non exhaustive) list of some useful documentations you could want to use [Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html), [Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html), [LayerNorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html), [activation functions](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity). Justify your choice of architecture. **[5 pts]**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gcw2l7QOLhif"
      },
      "outputs": [],
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        # ****START CODE****\n",
        "\n",
        "        # ****END CODE****\n",
        "\n",
        "    def forward(self, x):\n",
        "        # ****START CODE****\n",
        "\n",
        "        # ****END CODE****\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6dPrr6lNARI"
      },
      "outputs": [],
      "source": [
        "model = ConvNet()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-s0dBJ1zMefL"
      },
      "source": [
        "2.7. Initialize the training hyperparameters (optimizer, criterion, ...). Code the whole training loop, where the model is validated after each epoch, and where the essential information is output (training and validation loss and metric). For the metric you may want to use the [torchmetrics library](https://lightning.ai/docs/torchmetrics/stable/). **[5 pts]**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnzU5Q8nMd_9"
      },
      "outputs": [],
      "source": [
        "# ****START CODE****\n",
        "lr =\n",
        "num_epochs =\n",
        "optimizer =\n",
        "criterion =\n",
        "metric =\n",
        "# ****END CODE****"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47y4eKt8lH0Z"
      },
      "source": [
        "Train model and validate it after each epoch. Feel free to use a GPU if you're training on colab to speed up your training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Znowzr58NmtJ"
      },
      "outputs": [],
      "source": [
        "# ****START CODE****\n",
        "\n",
        "# ****END CODE****"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gb5rv7OfN5cx"
      },
      "source": [
        "2.8. Validate your model, show that it is not overfitting. Justify your choice of metric. Answer this either with code or Markdown (or both). **[3 pts]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8892z__OSFS"
      },
      "source": [
        "`ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qz6CR0NTOTzM"
      },
      "source": [
        "2.9. Try to optimize three hyperparameters (the learning rate, the batch size and the number of layers in your CNN model), does it improve the performance of your model? Anwser with a graph and comment on the result. Answer this with code and Markdown. **[8 pts]**\n",
        "\n",
        "To do so, use bayesian optimization to find the best set of hyperparameters using the library `scikit-optimize`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sR2sKRBKQ9JY"
      },
      "source": [
        "`ANSWER HERE`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3iO5k5oXYGyT"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-optimize # Run this cell to import the library in colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vS0Le5J9lrOB"
      },
      "outputs": [],
      "source": [
        "from skopt import gp_minimize\n",
        "from skopt.utils import use_named_args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfL3_nBohCn4"
      },
      "outputs": [],
      "source": [
        "# Retrieve the best set of hyperparameters using bayesian optimization.\n",
        "# Declare search space for your set of hyperparameters (you may take a look here: https://scikit-optimize.github.io/stable/modules/space.html#space)\n",
        "# ****START CODE****\n",
        "dimensions = [] # list of your search spaces\n",
        "parameters_default_values = [] # default value for each parameter for initialization\n",
        "# ****END CODE****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TH0J6f0ul7Q8"
      },
      "outputs": [],
      "source": [
        "# Create a function that take as input your set of hyperparameters and return a score to be minimized (choose wisely your scoring function)\n",
        "\n",
        "@use_named_args(dimensions=dimensions)\n",
        "def fit_opt():\n",
        "    # ****START CODE****\n",
        "\n",
        "    # ****END CODE****\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BJv8j5ml_qK"
      },
      "outputs": [],
      "source": [
        "# Use gp_minize to retrieve the optimal values (you may take a look here: https://scikit-optimize.github.io/stable/modules/generated/skopt.gp_minimize.html?highlight=gp_minimize#skopt.gp_minimize)\n",
        "\n",
        "gp_result = gp_minimize(\n",
        "    # ****START CODE****\n",
        "\n",
        "    # ****END CODE****\n",
        "    )\n",
        "\n",
        "print(f\"Optimal set of parameters found at iteration {np.argmin(gp_result.func_vals)}\")\n",
        "print(gp_result.x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwOO_DytqktF"
      },
      "source": [
        "2.10. OPTIONAL QUESTION. Implement a ViT and compare the results obtained in the previous section in a table. **[10 extra pts]**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gqt115rkqj5k"
      },
      "outputs": [],
      "source": [
        "# ****START CODE****\n",
        "\n",
        "# ****END CODE****"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoI3IQwMk1A-"
      },
      "source": [
        "2.11. With the exception of using Saliency maps, use one other interpretability method you listed in part 1.9 to investigate how your model made its classifications. **[12 pts]**\n",
        "\n",
        "How does your chosen method probe the classifications of your model? Do the results make sense?\n",
        "\n",
        "With respect to the code block below, saliency maps are useful in interpreting the decisions of CNNs. However, they have some limitations. After completing and running the code block below, list some of these limitations, given the results you observe on applying saliency maps to your images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IDOQ4m29Yu_"
      },
      "source": [
        "`ANSWER HERE`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzjgsTYmGJy2"
      },
      "outputs": [],
      "source": [
        "## Code block to use saliency maps\n",
        "\n",
        "### START CODE\n",
        "# Choose a particular image and corresponding label in which to investigate the classifications of the network\n",
        "image =\n",
        "label =\n",
        "\n",
        "preprocess = transforms.Compose([\n",
        "              ]) ### Here put the transforms to be applied\n",
        "\n",
        "input_tensor = preprocess(image).unsqueeze(0)  # Add batch dimension\n",
        "### END CODE\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Set the requires_grad attribute of the input tensor to True for gradients\n",
        "input_tensor.requires_grad_(True)\n",
        "\n",
        "# Forward pass to get the model prediction\n",
        "### START CODE\n",
        "output =\n",
        "### END CODE\n",
        "\n",
        "# Choose the class index for which you want to visualize the saliency map\n",
        "class_index = torch.argmax(output)\n",
        "\n",
        "model.zero_grad()\n",
        "\n",
        "# Backward pass to get the gradients of the output w.r.t the input\n",
        "output[0, class_index].backward()\n",
        "\n",
        "# Get the gradients from the input tensor\n",
        "saliency_map = input_tensor.grad.squeeze(0).abs().cpu().numpy()\n",
        "\n",
        "# Normalize the saliency map for visualization (optional)\n",
        "saliency_map = saliency_map / saliency_map.max()\n",
        "\n",
        "normalized_saliency_map = (saliency_map - saliency_map.min()) / (saliency_map.max() - saliency_map.min())\n",
        "\n",
        "# Convert the saliency map back to a uint8 image format (0-255)\n",
        "saliency_map_image = np.uint8(255 * normalized_saliency_map)\n",
        "\n",
        "# Aggregate across the channels\n",
        "aggregate_saliency = saliency_map.sum(axis=0)\n",
        "\n",
        "# Plot the input image and its corresponding saliency map side by side\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "# Plot the input image\n",
        "axes[0].imshow(image)\n",
        "axes[0].set_title('Input Image')\n",
        "axes[0].axis('off')\n",
        "\n",
        "# Plot the saliency map\n",
        "axes[1].imshow(aggregate_saliency, cmap='jet', alpha=0.7)  # Overlay saliency map on the input image\n",
        "axes[1].imshow(image, alpha=0.3)  # Overlay input image for comparison\n",
        "axes[1].set_title('Saliency Map')\n",
        "axes[1].axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFeRvuzss1ND"
      },
      "source": [
        "## Section 3. Implementing a CycleGAN **[50 pt]**\n",
        "\n",
        "In this part, we will implement CycleGAN using PyTorch. We will train a model to translate images of apples to oranges and vice versa.\n",
        "\n",
        "The CycleGAN model is composed of two generators and two discriminators. The generators are responsible for translating images from one domain to another, while the discriminators are responsible for distinguishing between translated images and real images. The generators and discriminators are trained in an adversarial manner, where the generators try to fool the discriminators and the discriminators try to distinguish between real and fake images. You can see an overview of the CycleGAN model in the figure below:\n",
        "<img src=\"https://junyanz.github.io/CycleGAN/images/cyclegan_blogs.jpg\">\n",
        "\n",
        "You can refer to the [CycleGAN paper](https://arxiv.org/pdf/1703.10593.pdf) for more information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-7n4dvCPUzW"
      },
      "source": [
        "You can first retrieve the data by executing the following cells:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXWEK1Lls31h"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/datasets/download_cyclegan_dataset.sh?raw=true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BraJMViitB-5"
      },
      "outputs": [],
      "source": [
        "!mkdir datasets\n",
        "!bash ./download_cyclegan_dataset.sh?raw=true apple2orange"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "gzHEGl6qAGZm"
      },
      "source": [
        "### 1) Dataset creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRZWrAmMu_2I"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import random\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from PIL import Image\n",
        "from torch.optim import lr_scheduler\n",
        "from IPython.display import clear_output\n",
        "from torch.utils.data import Subset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "gNoZZg3VAGZn"
      },
      "source": [
        "Some useful functions that we will use later:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YiF-ruAAGZo"
      },
      "outputs": [],
      "source": [
        "def denormalize(images, std=0.5, mean=0.5):\n",
        "    # For plot\n",
        "    images = (images * std) + mean\n",
        "    return images\n",
        "\n",
        "def deprocess(input_tensor):\n",
        "    if len(input_tensor.shape) == 3:\n",
        "        return np.transpose(denormalize(input_tensor.to(device).cpu()), (1, 2, 0))\n",
        "    elif len(input_tensor.shape) == 4:\n",
        "        return np.transpose(denormalize(input_tensor.to(device).cpu()), (0, 2, 3, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Pk0oV6foAGZp"
      },
      "source": [
        "3.1.1. You will now implement a simple dataset class in order to load the images. The dataset class should load the images from the dataset folder and apply the input transformations **[1 pt]**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0vcJQeDAGZr"
      },
      "outputs": [],
      "source": [
        "class GeneratorDataset(data.Dataset):\n",
        "\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        # ****START CODE****\n",
        "\n",
        "        # ****END CODE****\n",
        "\n",
        "    def __len__(self):\n",
        "        # ****START CODE****\n",
        "\n",
        "        # ****END CODE****\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # ****START CODE****\n",
        "\n",
        "        # ****END CODE****"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "6GWlU08MAGZr"
      },
      "source": [
        "We will now create the dataset objects for the training and testing sets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPT97Yz5AGZs"
      },
      "outputs": [],
      "source": [
        "DATASET = 'apple2orange'\n",
        "DATASET_PATH = os.path.join(\"datasets\", DATASET) # Dataset path\n",
        "OUTPUT_PATH = 'outputs'\n",
        "base_logdir = os.path.join(\"logs\", 'pytorch') # Sets up a log directory.\n",
        "RESIZE_SHAPE = 128 # Resized image size for faster training\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Create the dataset objects\n",
        "preprocess_train_transformations = transforms.Compose([\n",
        "                               transforms.Resize(RESIZE_SHAPE),\n",
        "                               transforms.RandomHorizontalFlip(p=0.5),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                           ])\n",
        "\n",
        "preprocess_test_transformations = transforms.Compose([\n",
        "                               transforms.Resize(RESIZE_SHAPE),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                           ])\n",
        "\n",
        "train_data_X = GeneratorDataset(root_dir=os.path.join(DATASET_PATH, \"trainA\"),\n",
        "                           transform=preprocess_train_transformations)\n",
        "\n",
        "train_data_Y = GeneratorDataset(root_dir=os.path.join(DATASET_PATH, \"trainB\"),\n",
        "                           transform=preprocess_train_transformations)\n",
        "\n",
        "test_data_X = GeneratorDataset(root_dir=os.path.join(DATASET_PATH, \"testA\"),\n",
        "                           transform=preprocess_test_transformations)\n",
        "\n",
        "test_data_Y = GeneratorDataset(root_dir=os.path.join(DATASET_PATH, \"testB\"),\n",
        "                           transform=preprocess_test_transformations)\n",
        "\n",
        "print(\"Found {} images in {}\".format(len(train_data_X), 'trainA'))\n",
        "print(\"Found {} images in {}\".format(len(train_data_Y), 'trainB'))\n",
        "print(\"Found {} images in {}\".format(len(test_data_X), 'testA'))\n",
        "print(\"Found {} images in {}\".format(len(test_data_Y), 'testB'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "_9HAzbX7AGZu"
      },
      "source": [
        "In order to speed up the training process, we will use a subset of the training data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-uASQRrAGZu"
      },
      "outputs": [],
      "source": [
        "random.seed(2)\n",
        "\n",
        "N_IMAGES_TO_SAMPLE = 400\n",
        "\n",
        "indices_X = random.sample(range(len(train_data_X)), N_IMAGES_TO_SAMPLE)\n",
        "indices_Y = random.sample(range(len(train_data_Y)), N_IMAGES_TO_SAMPLE)\n",
        "\n",
        "train_data_X = Subset(train_data_X, indices_X)\n",
        "train_data_Y = Subset(train_data_Y, indices_Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "PzyCFcw7AGZu"
      },
      "source": [
        "### 2) Generator and Discriminator Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Y9K1-pjsAGZu"
      },
      "source": [
        "3.2.1) We will now implement the backbone for the generator and the discriminator. You are asked to complete the code for the ResidualBlock of the generator backbone **[4 pt]**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4oG8yS_Nvaie"
      },
      "outputs": [],
      "source": [
        "##############################\n",
        "#           RESNET\n",
        "##############################\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_features):\n",
        "        # ****START CODE****\n",
        "\n",
        "        # ****END CODE****\n",
        "\n",
        "    def forward(self, x):\n",
        "        # ****START CODE****\n",
        "\n",
        "        # ****END CODE****\n",
        "\n",
        "\n",
        "class GeneratorResNet(nn.Module):\n",
        "    def __init__(self, input_channel, n_blocks, filters, output_channel):\n",
        "        super(GeneratorResNet, self).__init__()\n",
        "\n",
        "        # Initial convolution block\n",
        "        model = [\n",
        "            nn.ReflectionPad2d(input_channel),\n",
        "            nn.Conv2d(input_channel, filters, 7),\n",
        "            nn.InstanceNorm2d(filters),\n",
        "            nn.ReLU(inplace=True),\n",
        "        ]\n",
        "        in_features = filters\n",
        "\n",
        "        # Downsampling\n",
        "        for _ in range(2):\n",
        "            filters *= 2\n",
        "            model += [\n",
        "                nn.Conv2d(in_features, filters, 3, stride=2, padding=1),\n",
        "                nn.InstanceNorm2d(filters),\n",
        "                nn.ReLU(inplace=True),\n",
        "            ]\n",
        "            in_features = filters\n",
        "\n",
        "        # Residual blocks\n",
        "        for _ in range(n_blocks):\n",
        "            model += [ResidualBlock(filters)]\n",
        "\n",
        "        # Upsampling\n",
        "        for _ in range(2):\n",
        "            filters //= 2\n",
        "            model += [\n",
        "                nn.Upsample(scale_factor=2),\n",
        "                nn.Conv2d(in_features, filters, 3, stride=1, padding=1),\n",
        "                nn.InstanceNorm2d(filters),\n",
        "                nn.ReLU(inplace=True),\n",
        "            ]\n",
        "            in_features = filters\n",
        "\n",
        "        # Output layer\n",
        "        model += [nn.ReflectionPad2d(output_channel), nn.Conv2d(filters, output_channel, 7), nn.Tanh()]\n",
        "\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "##############################\n",
        "#        Discriminator\n",
        "##############################\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_channel, filters):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        def discriminator_block(in_filters, out_filters, normalize=True):\n",
        "            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n",
        "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
        "            if normalize:\n",
        "                layers.append(nn.InstanceNorm2d(out_filters))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            *discriminator_block(input_channel, filters, normalize=False),\n",
        "            *discriminator_block(filters, filters * 2),\n",
        "            *discriminator_block(filters * 2, filters * 4),\n",
        "            *discriminator_block(filters * 4, filters *8),\n",
        "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
        "            nn.Conv2d(filters *8, 1, 4, padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        return self.model(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "MggkB25AAGZv"
      },
      "source": [
        "We will now instantiate the generator and discriminator models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwcZzA9UAGZv"
      },
      "outputs": [],
      "source": [
        "G_XtoY = GeneratorResNet(input_channel=3, output_channel=3, filters=64, n_blocks=9).to(device)\n",
        "G_YtoX = GeneratorResNet(input_channel=3, output_channel=3, filters=64, n_blocks=9).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbA2oKwyAGZv"
      },
      "outputs": [],
      "source": [
        "Dx = Discriminator(input_channel=3, filters=64).to(device)\n",
        "Dy = Discriminator(input_channel=3, filters=64).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZSihjHs0gIM"
      },
      "source": [
        "3.2.2) You will now implement a function to randomly initialize the weights for all the convolutional layers in the generators and the discriminators with values sampled from a normal distribution (mean=0.0, std=0.02), and initialize their bias to 0.0. You need to complete the code of the *weights_init_normal* function and apply it to the models **[5 pt]**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RnWeQom0vLE"
      },
      "outputs": [],
      "source": [
        "def weights_init_normal(m):\n",
        "    # ****START CODE****\n",
        "\n",
        "    # ****END CODE****\n",
        "\n",
        "# ============================\n",
        "# Initialize the values of the models\n",
        "# ============================\n",
        "# Initialize the values of the two generators\n",
        "# ****START CODE****\n",
        "\n",
        "# ****END CODE****\n",
        "# Initialize the values of the two discriminators\n",
        "# ****START CODE****\n",
        "\n",
        "# ****END CODE****"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BjPyRGt6EDL"
      },
      "source": [
        "### 3) Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "MzT2ptrnAGZw"
      },
      "source": [
        "We will define the hyperparameters used for training our CycleGAN model. The model should run on the T4 GPU provided by Google Colab. You may need to adjust the batch size to fit the model on other GPUs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XbshCeHtKXp"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 10\n",
        "EPOCHs = 30\n",
        "SAVE_EVERY_N_EPOCH = 5\n",
        "LR = 0.0002\n",
        "BETAS = (0.5, 0.999)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Aj4nB4bUAGZx"
      },
      "source": [
        "We will now define the data loaders for the training and testing sets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIWKI41nAGZx"
      },
      "outputs": [],
      "source": [
        "train_image_loader_X = torch.utils.data.DataLoader(train_data_X, batch_size=BATCH_SIZE,\n",
        "                                                    shuffle=True, num_workers=0)\n",
        "train_image_loader_Y = torch.utils.data.DataLoader(train_data_Y, batch_size=BATCH_SIZE,\n",
        "                                         shuffle=True, num_workers=0)\n",
        "test_image_loader_X = torch.utils.data.DataLoader(test_data_X, batch_size=BATCH_SIZE,\n",
        "                                         shuffle=False, num_workers=0)\n",
        "test_image_loader_Y = torch.utils.data.DataLoader(test_data_Y, batch_size=BATCH_SIZE,\n",
        "                                         shuffle=False, num_workers=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "XMf6ZE1tAGZy"
      },
      "source": [
        "We will now extract some images from the test set to visualize the model's performance during training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0_3FXu3ysRd"
      },
      "outputs": [],
      "source": [
        "id_sample_X = np.where(test_data_X.filenames == \"n07740461_11391.jpg\")[0][0]\n",
        "id_sample_Y = np.where(test_data_Y.filenames == \"n07749192_10081.jpg\")[0][0]\n",
        "\n",
        "sample_X = test_data_X[id_sample_X]\n",
        "sample_Y = test_data_Y[id_sample_Y]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A61RgkUFyxax"
      },
      "outputs": [],
      "source": [
        "plt.subplot(121)\n",
        "plt.title('X')\n",
        "plt.imshow(deprocess(sample_X))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_Rcyld0zIk8"
      },
      "outputs": [],
      "source": [
        "plt.subplot(121)\n",
        "plt.title('Y')\n",
        "plt.imshow(deprocess(sample_Y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "ldr-TlvSAGZy"
      },
      "source": [
        "3.3.1. You will now define the optimizers and schedulers for the generator and discriminator models **[3 pts]**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxNHU3hWzMSO"
      },
      "outputs": [],
      "source": [
        "# ****START CODE****\n",
        "\n",
        "# ****END CODE****"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "l7HyFbj3AGZz"
      },
      "source": [
        "We will now implement the different loss functions used in CycleGANs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZKi9DRGAGZ0"
      },
      "outputs": [],
      "source": [
        "SOFT_FAKE_LABEL_RANGE =  [0.0, 0.3] # The label of fake label will be generated within this range.\n",
        "SOFT_REAL_LABEL_RANGE = [0.7, 1.2] # The label of real label will be generated within this range."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "vunbmWLAAGZ0"
      },
      "source": [
        "The discriminator loss is defined by:\n",
        "\\begin{equation}\n",
        "\\mathcal{L}_{D} = \\frac{1}{2} (\\mathbb{E}_{y \\sim p_{data}(y)}[(D_Y(y) - r_2)^2] + \\mathbb{E}_{x \\sim p_{data}(x)}[(D_Y(G_{XY}(x))-r_1)^2]) + \\frac{1}{2} (\\mathbb{E}_{x \\sim p_{data}(x)}[(D_X(x) - r_2)^2] + \\mathbb{E}_{y \\sim p_{data}(y)}[(D_X(G_{YX}(y))-r_1)^2])\n",
        "\\end{equation}\n",
        "with $p_{data}(x)$ being the distribution of images from the first domain, $p_{data}(y)$ being the distribution of images from the second domain, $G_{XY}$ and $G_{YX}$ being the two generators, $D_X$ and $D_Y$ the two discriminators, and $r_1$ and $r_2$ being the soft fake and real labels chosen from a uniform distribution within the ranges $[0.0, 0.3]$ and $[0.7, 1.2]$ respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "cgQrpzBHAGZ1"
      },
      "source": [
        "3.3.2. You will now implement the discriminator loss function **[4 pts]**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjSnU8uLAGZ1"
      },
      "outputs": [],
      "source": [
        "def discriminator_loss(real_image, generated_image):\n",
        "    # ****START CODE****\n",
        "\n",
        "    # ****END CODE****\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "BpTlQl4vAGZ1"
      },
      "source": [
        "The generator loss is defined by:\n",
        "\\begin{equation}\n",
        "\\mathcal{L}_{G} = \\mathbb{E}_{x \\sim p_{data}(x)}(D_Y(G_{XY}(x)) - r_2)^2 + \\mathbb{E}_{y \\sim p_{data}(y)}(D_X(G_{YX}(y)) - r_2)^2\n",
        "\\end{equation}\n",
        "with $p_{data}(x)$ being the distribution of images from the first domain, $p_{data}(y)$ being the distribution of images from the second domain, $G_{XY}$ and $G_{YX}$ being the two generators, $D_X$ and $D_Y$ the two discriminators, and $r_2$ being the soft real label chosen from a uniform distribution within the range $[0.7, 1.2]$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "7DewJqhkAGZ1"
      },
      "source": [
        "3.2.3. You will now implement the generator loss function for a domain **[4 pts]**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVOo5BEjAGZ1"
      },
      "outputs": [],
      "source": [
        "def generator_loss(generated_image):\n",
        "    # ****START CODE****\n",
        "\n",
        "    # ****END CODE****\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "XpuTKLY7AGZ2"
      },
      "source": [
        "In addition to the traditional loss functions used in GANs, CycleGANs also use two additional loss functions: cycle consistency loss and identity loss. We will use the same $\\lambda$ for the two losses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYZvL4pNAGZ2"
      },
      "outputs": [],
      "source": [
        "LAMBDA = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "1PVM9LAPAGZ2"
      },
      "source": [
        "The cycle consistency loss is defined by:\n",
        "\\begin{equation}\n",
        "\\mathcal{L}_{cyc} = \\lambda\\mathbb{E}_{x \\sim p_{data}(x)}[||x - G_{YX}(G_{XY}(x))||_1] + \\lambda\\mathbb{E}_{y \\sim p_{data}(y)}[||y - G_{XY}(G_{YX}(y))||_1]\n",
        "\\end{equation}\n",
        "with $p_{data}(x)$ being the distribution of images from the first domain, $p_{data}(y)$ being the distribution of images from the second domain, $G_{XY}$ and $G_{YX}$ being the two generators and $\\lambda$ being the weight for the cycle consistency loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "SVMWBOi9AGZ2"
      },
      "source": [
        "3.3.4. You will now implement the cycle consistency loss function **[4 pts]**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZnmI7d4AGZ2"
      },
      "outputs": [],
      "source": [
        "def cycle_consistency_loss(real_image, cycled_image):\n",
        "    # ****START CODE****\n",
        "\n",
        "    # ****END CODE****\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "3dx5puXsAGZ2"
      },
      "source": [
        "The identity loss is defined by:\n",
        "\\begin{equation}\n",
        "\\mathcal{L}_{id} = \\frac{1}{2}\\lambda\\mathbb{E}_{x \\sim p_{data}(x)}[||G_{YX}(x) - x||_1] + \\frac{1}{2}\\lambda\\mathbb{E}_{y \\sim p_{data}(y)}[||G_{XY}(y) - y||_1]\n",
        "\\end{equation}\n",
        "with $p_{data}(x)$ being the distribution of images from the first domain, $p_{data}(y)$ being the distribution of images from the second domain, $G_{XY}$ and $G_{YX}$ being the two generators, and $\\lambda$ being the weight for the identity loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdXZtjHr9716"
      },
      "source": [
        "3.3.5. You will now implement the identity loss function **[4 pts]**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6Za9t-9zWcN"
      },
      "outputs": [],
      "source": [
        "def identity_loss(real_image, generated_image):\n",
        "    # ****START CODE****\n",
        "\n",
        "    # ****END CODE****\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "pHk4HILlAGZ3"
      },
      "source": [
        "The total generator loss is defined by:\n",
        "\\begin{equation}\n",
        "\\mathcal{L}_{G_{tot}} = \\mathcal{L}_{G} + \\mathcal{L}_{cyc} + \\mathcal{L}_{id}\n",
        "\\end{equation}\n",
        "with $\\mathcal{L}_{G_X}$ and $\\mathcal{L}_{G_Y}$ being the generator loss for the two domains, $\\mathcal{L}_{cyc}$ being the cycle consistency loss and $\\mathcal{L}_{id}$ being the identity loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "yUAjBJYrAGZ3"
      },
      "source": [
        "We will now set the checkpoint path for saving the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbfO_5Zw0Hfd"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = os.path.join(\"checkpoints\", 'pytorch', DATASET, )\n",
        "\n",
        "if not os.path.exists(checkpoint_path):\n",
        "    os.makedirs(checkpoint_path)\n",
        "\n",
        "def save_training_checkpoint(epoch):\n",
        "    state_dict = {\n",
        "    'G_XtoY':G_XtoY.state_dict(),\n",
        "    'G_YtoX':G_YtoX.state_dict(),\n",
        "    'Dx':Dx.state_dict(),\n",
        "    'Dy':Dy.state_dict(),\n",
        "    'G_XtoY_optimizer':G_XtoY_optimizer.state_dict(),\n",
        "    'G_YtoX_optimizer':G_YtoX_optimizer.state_dict(),\n",
        "    'Dx_optimizer':Dx_optimizer.state_dict(),\n",
        "    'Dy_optimizer':Dy_optimizer.state_dict(),\n",
        "    'epoch': epoch\n",
        "    }\n",
        "\n",
        "    save_path = os.path.join(checkpoint_path, 'training-checkpoint')\n",
        "    torch.save(state_dict, save_path)\n",
        "\n",
        "# if a checkpoint exists, restore the latest checkpoint.\n",
        "if os.path.isfile(os.path.join(checkpoint_path, 'training-checkpoint')):\n",
        "    checkpoint = torch.load(os.path.join(checkpoint_path, 'training-checkpoint'))\n",
        "    G_XtoY.load_state_dict(checkpoint['G_XtoY'])\n",
        "    G_YtoX.load_state_dict(checkpoint['G_YtoX'])\n",
        "    Dx.load_state_dict(checkpoint['Dx'])\n",
        "    Dy.load_state_dict(checkpoint['Dy'])\n",
        "    G_XtoY_optimizer.load_state_dict(checkpoint['G_XtoY_optimizer'])\n",
        "    G_YtoX_optimizer.load_state_dict(checkpoint['G_YtoX_optimizer'])\n",
        "    Dx_optimizer.load_state_dict(checkpoint['Dx_optimizer'])\n",
        "    Dy_optimizer.load_state_dict(checkpoint['Dy_optimizer'])\n",
        "    CURRENT_EPOCH = checkpoint['epoch']\n",
        "    print ('Latest checkpoint of epoch {} restored!!'.format(CURRENT_EPOCH))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "hmhIPQXsAGZ4"
      },
      "source": [
        "3.3.6. You will now implement a function to generate in the other modality a given *test_input* image using a trained generator **[2pts]**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWZZwnRN0Iq7"
      },
      "outputs": [],
      "source": [
        "def generate_images(model, test_input):\n",
        "    # ****START CODE****\n",
        "\n",
        "    # ****END CODE****"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "bqx4yFduAGZ5"
      },
      "source": [
        "3.3.7. You will now complete the following code to perform the training process **[10 pts]**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5_bSMT6zbMo"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "training_steps = np.ceil((min(len(train_data_X), len(train_data_Y)) / BATCH_SIZE)).astype(int)\n",
        "\n",
        "for epoch in range(1, EPOCHs + 1):\n",
        "    start = time.time()\n",
        "    print('Start of epoch %d' % (epoch,))\n",
        "    # Reset dataloader\n",
        "    iter_train_image_X = iter(train_image_loader_X)\n",
        "    iter_train_image_Y = iter(train_image_loader_Y)\n",
        "    # Initialize losses\n",
        "    G_XtoY_loss_mean = 0\n",
        "    G_YtoX_loss_mean = 0\n",
        "    Dx_loss_mean = 0\n",
        "    Dy_loss_mean = 0\n",
        "    for step in range(training_steps):\n",
        "\n",
        "        real_image_X = next(iter_train_image_X).to(device)\n",
        "        real_image_Y = next(iter_train_image_Y).to(device)\n",
        "\n",
        "        # ============================\n",
        "        # Compute the discriminator loss\n",
        "        # ============================\n",
        "        # Generate fake images for discriminators\n",
        "        # ****START CODE****\n",
        "\n",
        "        # ****END CODE****\n",
        "\n",
        "\n",
        "        # Compute the discriminator loss using the latest fake images\n",
        "        # ****START CODE****\n",
        "\n",
        "        # ****END CODE****\n",
        "\n",
        "        # ============================\n",
        "        # Update discriminators\n",
        "        # ============================\n",
        "        # ****START CODE****\n",
        "\n",
        "        # ****END CODE****\n",
        "\n",
        "        # ============================\n",
        "        # Compute the generator loss\n",
        "        # ============================\n",
        "        # Generate fake images for generators\n",
        "        # ****START CODE****\n",
        "\n",
        "        # ****END CODE****\n",
        "\n",
        "        # Compute the generator loss using the latest fake images\n",
        "        # ****START CODE****\n",
        "\n",
        "        # ****END CODE****\n",
        "\n",
        "        # ============================\n",
        "        # Compute the cycle consistency loss\n",
        "        # ============================\n",
        "        # Generate cycled images using the latest fake images\n",
        "        # ****START CODE****\n",
        "\n",
        "        # ****END CODE****\n",
        "\n",
        "        # Compute the cycle consistency loss using the latest cycled images\n",
        "        # ****START CODE****\n",
        "\n",
        "        # ****END CODE****\n",
        "\n",
        "        # ============================\n",
        "        # Compute the identity loss\n",
        "        # ============================\n",
        "        # Generate identity images using the latest fake images\n",
        "        # ****START CODE****\n",
        "\n",
        "        # ****END CODE****\n",
        "        # Compute the identity loss using the latest identity images\n",
        "        # ****START CODE****\n",
        "\n",
        "        # ****END CODE****\n",
        "\n",
        "        # ============================\n",
        "        # Combine all generator losses\n",
        "        # ============================\n",
        "        # ****START CODE****\n",
        "\n",
        "        # ****END CODE****\n",
        "        # ============================\n",
        "        # Update generators\n",
        "        # ============================\n",
        "        # ****START CODE****\n",
        "\n",
        "        # ****END CODE****\n",
        "\n",
        "        # Add losses\n",
        "        # ****START CODE****\n",
        "\n",
        "        # ****END CODE****\n",
        "\n",
        "        if step % 10 == 0:\n",
        "            print ('.', end='')\n",
        "\n",
        "    clear_output(wait=True)\n",
        "    # ============================\n",
        "    # Print loss values at the end of an epoch\n",
        "    # ============================\n",
        "    # ****START CODE****\n",
        "\n",
        "    # ****END CODE****\n",
        "\n",
        "    # ============================\n",
        "    # Using consistent images (sample_X and sample_Y), plot the progress of the training using both generators\n",
        "    # ============================\n",
        "    # ****START CODE****\n",
        "\n",
        "    # ****END CODE****\n",
        "\n",
        "    # ============================\n",
        "    # Save the checkpoint for every SAVE_EVERY_N_EPOCH epoch\n",
        "    # ============================\n",
        "    if epoch % SAVE_EVERY_N_EPOCH == 0:\n",
        "        # ****START CODE****\n",
        "\n",
        "        # ****END CODE****\n",
        "        print ('Saving checkpoint for epoch {} at {}'.format(epoch,\n",
        "                                                             checkpoint_path))\n",
        "\n",
        "    print ('Time taken for epoch {} is {} sec\\n'.format(epoch,\n",
        "                                                      time.time()-start))\n",
        "    gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "miIYZhIyAGZ6"
      },
      "source": [
        "### 4) Testing\n",
        "\n",
        "3.4.1. You will now generate images using the trained models, what do you observe from the generated images ? Discuss any anomalies and suggest possible solutions **[3 pts]**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flcMDaLR0dRU"
      },
      "outputs": [],
      "source": [
        "# ****START CODE****\n",
        "\n",
        "# ****END CODE****"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0406e626"
      },
      "source": [
        "### 5) Theory Questions **[6 pts]** (2 pts each):\n",
        "5.1. What are the key differences between a traditional GAN and a CycleGAN?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mO6k0DB9-HRX"
      },
      "source": [
        "`ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67CnQPTc9-92"
      },
      "source": [
        "5.2. How does the architecture of a CycleGAN ensure the preservation of key features in the image translation process?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Yjy4lOV-Q-J"
      },
      "source": [
        "`ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVWVfNab-Cp1"
      },
      "source": [
        "5.3. Discuss the role of cycle consistency loss in CycleGAN. Why is it important?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmL1T7GC-TGZ"
      },
      "source": [
        "`ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "914e5c6e"
      },
      "source": [
        "### 6) Optional Question\n",
        "6.1. Implement an additional feature in the CycleGAN model or play with the parameters to improve the performance of the model. **[10 extra pts]**\n",
        "\n",
        "It may include:\n",
        "*   Changing the backbone architecture for the generator and/or the discriminator (Unet style, ...)\n",
        "*   Changing the hyperparameters\n",
        "*   Adding new losses\n",
        "\n",
        "Illustrate the results you get with these improvements compared to the original implementation. Every implementation choices must be properly justified (in a few lines), or no points will be rewarded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgKqtsVXAGZ7"
      },
      "outputs": [],
      "source": [
        "# ****START CODE****\n",
        "\n",
        "# ****END CODE****"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.5 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "fd75362e27048f1ead3b65beb4812b1da3d387150557ce53b099093c32022a5e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
