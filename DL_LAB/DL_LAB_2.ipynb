{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LdvYJK8dGMGC"
   },
   "source": [
    "# Deep Learning Course: Lab Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "esEx2gjvGMGC"
   },
   "source": [
    "In this lab exercise you will:\n",
    "\n",
    "a) Solve a simple Linear Regression problem with PyTorch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTWw0X_9GMGC"
   },
   "source": [
    "# Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pquVrDEfGMGC"
   },
   "source": [
    "Apart from the Questions, there are instruction comments throughout the notebook as well as comments inside the code cells beginning with two hashtags (##). In addition, there are #**START CODE  /  #**END CODE comments indicating the startand ending of your code sections. Pay attention not to delete these comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YYYvNu6U9Hkf"
   },
   "source": [
    "#**Q1 Solve a simple Linear Regression problem**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tq-fv4XuUDyF"
   },
   "source": [
    "Find the linear relationship between a dependent variable 'y' and an independent variable 'x'.\n",
    "\n",
    "Equation: y = 4x + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PTFbA6ssqJKQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.11821625 9.50463696 1.44159613 9.48649447 3.11831452 4.23326449\n",
      " 8.27702594 4.09199136 5.49593688 0.27559113 7.53513109 5.38143313\n",
      " 3.29731716 7.88428703 3.03194829]\n"
     ]
    }
   ],
   "source": [
    "## create dummy data for training\n",
    "import numpy as np\n",
    "rng = np.random.default_rng(seed=1)\n",
    "x_train = rng.uniform(low=0.0, high=10.0, size=(15,))\n",
    "print(x_train)\n",
    "x_train = np.array(x_train, dtype=np.float32)\n",
    "x_train = x_train.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "V2Vg8txkqMds"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19.354649 ],\n",
       "       [32.51391  ],\n",
       "       [ 8.324789 ],\n",
       "       [32.45948  ],\n",
       "       [13.354943 ],\n",
       "       [16.699793 ],\n",
       "       [28.831078 ],\n",
       "       [16.275974 ],\n",
       "       [20.48781  ],\n",
       "       [ 4.8267736],\n",
       "       [26.605392 ],\n",
       "       [20.144299 ],\n",
       "       [13.891952 ],\n",
       "       [27.65286  ],\n",
       "       [13.095845 ]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##create the y values according to equation y = 3x + 4\n",
    "# *****START CODE\n",
    "\n",
    "y_train = 3 * x_train + 4\n",
    "\n",
    "# *****END CODE\n",
    "y_train = np.array(y_train, dtype=np.float32)\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Sm3rqZccllF"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "##convert x_train and y_train numpy arrays to pytorch tensors\n",
    "# *****START CODE\n",
    "x_train = torch.tensor(x_train)\n",
    "y_train = torch.tensor(y_train)\n",
    "# *****END CODE \n",
    "# tensor 是 pytorch张量，是执行数学运算、构建和训练神经网络以及进行科学计算的基本单位，与numpy数组类似"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4W5mVjpWBtAH"
   },
   "outputs": [],
   "source": [
    "class LinearRegression(torch.nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        ##Define a linear layer\n",
    "        # *****START CODE\n",
    "        self.linear = torch.nn.Linear(in_size, out_size)\n",
    "\n",
    "        # *****END CODE\n",
    "        #class LinearRegression(torch.nn.Module): 这里定义了一个名为LinearRegression的类，\n",
    "        # 它继承自torch.nn.Module。在PyTorch中，自定义的模型需要继承自nn.Module类，这是所有神经网络模块的基类。\n",
    "\n",
    "    def forward(self, x):\n",
    "        ##forward the input through the model\n",
    "        # *****START CODE\n",
    "\n",
    "        out = self.linear(x)\n",
    "\n",
    "        # *****END CODE\n",
    "        return out\n",
    "#总结来说，这段代码定义了一个线性回归模型，\n",
    "# 它包含一个线性层，可以根据输入特征进行线性变换以预测输出值。这种模型通常用于回归任务，其中目标是预测一个或多个连续值的输出。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6pNUFrD6B3qM"
   },
   "outputs": [],
   "source": [
    "##define input and output dimensions of the model\n",
    "# *****START CODE\n",
    "in_dim =    1   \n",
    "out_dim =   1   \n",
    "# *****END CODE\n",
    "\n",
    "LR = 0.01 \n",
    "epochs = 100\n",
    "\n",
    "model = LinearRegression(in_dim, out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "7OieSFIRB4lE"
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss() \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "U-o_W3s-qQc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 293.84429931640625\n",
      "epoch 1, loss 27.21700668334961\n",
      "epoch 2, loss 4.584229469299316\n",
      "epoch 3, loss 2.6454808712005615\n",
      "epoch 4, loss 2.4620118141174316\n",
      "epoch 5, loss 2.4275736808776855\n",
      "epoch 6, loss 2.4059364795684814\n",
      "epoch 7, loss 2.3855409622192383\n",
      "epoch 8, loss 2.365408182144165\n",
      "epoch 9, loss 2.3454549312591553\n",
      "epoch 10, loss 2.3256683349609375\n",
      "epoch 11, loss 2.3060483932495117\n",
      "epoch 12, loss 2.2865946292877197\n",
      "epoch 13, loss 2.2673046588897705\n",
      "epoch 14, loss 2.248178720474243\n",
      "epoch 15, loss 2.229212760925293\n",
      "epoch 16, loss 2.210407257080078\n",
      "epoch 17, loss 2.1917600631713867\n",
      "epoch 18, loss 2.1732709407806396\n",
      "epoch 19, loss 2.154938220977783\n",
      "epoch 20, loss 2.1367580890655518\n",
      "epoch 21, loss 2.1187336444854736\n",
      "epoch 22, loss 2.100858688354492\n",
      "epoch 23, loss 2.083136796951294\n",
      "epoch 24, loss 2.0655624866485596\n",
      "epoch 25, loss 2.048137903213501\n",
      "epoch 26, loss 2.03085994720459\n",
      "epoch 27, loss 2.013728141784668\n",
      "epoch 28, loss 1.9967402219772339\n",
      "epoch 29, loss 1.9798941612243652\n",
      "epoch 30, loss 1.9631937742233276\n",
      "epoch 31, loss 1.9466325044631958\n",
      "epoch 32, loss 1.9302101135253906\n",
      "epoch 33, loss 1.913927435874939\n",
      "epoch 34, loss 1.8977817296981812\n",
      "epoch 35, loss 1.881772518157959\n",
      "epoch 36, loss 1.8658974170684814\n",
      "epoch 37, loss 1.8501559495925903\n",
      "epoch 38, loss 1.834548830986023\n",
      "epoch 39, loss 1.8190734386444092\n",
      "epoch 40, loss 1.8037272691726685\n",
      "epoch 41, loss 1.788509726524353\n",
      "epoch 42, loss 1.7734239101409912\n",
      "epoch 43, loss 1.7584620714187622\n",
      "epoch 44, loss 1.743627667427063\n",
      "epoch 45, loss 1.7289186716079712\n",
      "epoch 46, loss 1.7143343687057495\n",
      "epoch 47, loss 1.699872374534607\n",
      "epoch 48, loss 1.6855312585830688\n",
      "epoch 49, loss 1.6713122129440308\n",
      "epoch 50, loss 1.657213568687439\n",
      "epoch 51, loss 1.6432337760925293\n",
      "epoch 52, loss 1.6293712854385376\n",
      "epoch 53, loss 1.6156260967254639\n",
      "epoch 54, loss 1.6019965410232544\n",
      "epoch 55, loss 1.588481068611145\n",
      "epoch 56, loss 1.575081706047058\n",
      "epoch 57, loss 1.5617941617965698\n",
      "epoch 58, loss 1.5486189126968384\n",
      "epoch 59, loss 1.5355559587478638\n",
      "epoch 60, loss 1.5226014852523804\n",
      "epoch 61, loss 1.5097569227218628\n",
      "epoch 62, loss 1.497021198272705\n",
      "epoch 63, loss 1.4843924045562744\n",
      "epoch 64, loss 1.4718685150146484\n",
      "epoch 65, loss 1.4594515562057495\n",
      "epoch 66, loss 1.4471397399902344\n",
      "epoch 67, loss 1.4349327087402344\n",
      "epoch 68, loss 1.422828197479248\n",
      "epoch 69, loss 1.4108253717422485\n",
      "epoch 70, loss 1.398923397064209\n",
      "epoch 71, loss 1.3871217966079712\n",
      "epoch 72, loss 1.375420331954956\n",
      "epoch 73, loss 1.3638174533843994\n",
      "epoch 74, loss 1.3523128032684326\n",
      "epoch 75, loss 1.3409041166305542\n",
      "epoch 76, loss 1.329592227935791\n",
      "epoch 77, loss 1.3183761835098267\n",
      "epoch 78, loss 1.3072539567947388\n",
      "epoch 79, loss 1.2962266206741333\n",
      "epoch 80, loss 1.2852911949157715\n",
      "epoch 81, loss 1.274448275566101\n",
      "epoch 82, loss 1.2636971473693848\n",
      "epoch 83, loss 1.253036618232727\n",
      "epoch 84, loss 1.2424654960632324\n",
      "epoch 85, loss 1.2319846153259277\n",
      "epoch 86, loss 1.2215908765792847\n",
      "epoch 87, loss 1.2112860679626465\n",
      "epoch 88, loss 1.2010680437088013\n",
      "epoch 89, loss 1.19093656539917\n",
      "epoch 90, loss 1.1808888912200928\n",
      "epoch 91, loss 1.1709271669387817\n",
      "epoch 92, loss 1.161049485206604\n",
      "epoch 93, loss 1.1512548923492432\n",
      "epoch 94, loss 1.1415425539016724\n",
      "epoch 95, loss 1.1319124698638916\n",
      "epoch 96, loss 1.1223645210266113\n",
      "epoch 97, loss 1.1128956079483032\n",
      "epoch 98, loss 1.103507161140442\n",
      "epoch 99, loss 1.0941989421844482\n"
     ]
    }
   ],
   "source": [
    "#model.train()\n",
    "for epoch in range(epochs):\n",
    "    # Set all gradients to zero\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # feed the inputs to the model, and get the outputs\n",
    "    # *****START CODE\n",
    "    outputs = model(x_train )\n",
    "    # *****END CODE\n",
    "\n",
    "    # calculate loss for the predicted output\n",
    "    # *****START CODE\n",
    "    loss = criterion(outputs, y_train)\n",
    "    # *****END CODE\n",
    "\n",
    "    # get gradients w.r.t to parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    print('epoch {}, loss {}'.format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "auS_ZOKCfSBg"
   },
   "source": [
    "Create random test data and evaluate the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qdSkAIacVYfi"
   },
   "outputs": [],
   "source": [
    "## create 'x' values for testing\n",
    "# *****START CODE\n",
    "\n",
    "\n",
    "\n",
    "# *****END CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "32QizCbEXotQ"
   },
   "outputs": [],
   "source": [
    "##create the corresponding testing 'y' values\n",
    "# *****START CODE\n",
    "\n",
    "\n",
    "\n",
    "# *****END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-w-dADCg1Gx"
   },
   "source": [
    "Evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "navbs-J_qVaz"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad(): # we don't need gradients in the testing phase\n",
    "        predicted = model(torch.from_numpy(x_test)).data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "js9B__sUsyfK"
   },
   "outputs": [],
   "source": [
    "##Print the predicted values and the true values\n",
    "print('Predicted: ')\n",
    "# *****START CODE\n",
    "print( )\n",
    "# *****END CODE\n",
    "\n",
    "print('True y values: ')\n",
    "# *****START CODE\n",
    "print( )\n",
    "# *****END CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I9KNv6B3tFoa"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "##plot the predicted values along with the true values\n",
    "# *****START CODE\n",
    "plt.plot(  ,  , 'go', label='True data', alpha=0.5)\n",
    "plt.plot(  ,  , '--', label='Predictions', alpha=0.5)\n",
    "# *****END CODE\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Q2 Classification with an MLP**\n",
    "\n",
    "We are going to work with a dataset of 8x8 images of digits, and our goal will be to classify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1797 images in this dataset of size (8, 8).\n"
     ]
    }
   ],
   "source": [
    "digits_dataset = load_digits()\n",
    "digits_x = digits_dataset.images / 16\n",
    "digits_y = digits_dataset.target\n",
    "print(f'There are {len(digits_x)} images in this dataset of size {digits_x.shape[1:]}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, val_x, train_y, val_y = train_test_split(digits_x, digits_y, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_digit(x, y, idx):\n",
    "    x = x[idx]\n",
    "    plt.imshow(x)\n",
    "    plt.title(y[idx])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_digit(digits_x, digits_y, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Part 2\n",
    "\n",
    "You will now implement a simple 2 layer MLP model that you will train using gradient descent algorithm.\n",
    "\n",
    "Here are a few reminders:\n",
    "\n",
    "Let $\\mathcal{M}$ be a 2 a layer MLP model trained on $X$ to predict $y$ with categorical cross-entropy loss. We have:\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathcal{L} = - \\sum_{i=1}^{n} y_ilog(\\hat{y}_i)\n",
    "\\end{align}\n",
    "\n",
    "$\\mathcal{M}$ can be decomposed as:\n",
    "\n",
    "\\begin{align}\n",
    "    \\hat{y} &= \\sigma_2(Z_2) \\\\\n",
    "    Z_2 &= W_2Y_1 + b_2\n",
    "\\end{align}\n",
    "\n",
    "with $\\sigma_2$ the softmax activation function.\n",
    "\n",
    "and:\n",
    "\n",
    "\\begin{align}\n",
    "    Y_1 &= \\sigma_1(Z_1) \\\\\n",
    "    Z_1 &= W_1X + b_1\n",
    "\\end{align}\n",
    "\n",
    "with $\\sigma_1$ the sigmoid activation function.\n",
    "\n",
    "### Question\n",
    "\n",
    "Compute the gradient of the softmax activation function used in the output layer for categorical cross-entropy loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Question\n",
    "\n",
    "Compute the gradient of the categorical cross-entropy loss function $\\mathcal{L}$ with respect to the pre-activation outputs $Z_2$ of the second layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Question\n",
    "\n",
    "Based on your understanding of backpropagation, derive the expressions for gradients of the loss function with respect to weights $W_2$ and biases $b_2$ in the second layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Question\n",
    "\n",
    "Recall the gradient of sigmoid activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Question\n",
    "\n",
    "Conclude the expression  for the gradients of the loss function with respect to weights $W_1$, and biases $b_1$ of the first layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Complete now the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#一个MLP多层感知机\n",
    "class SimpleMLP:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # *****START CODE\n",
    "\n",
    "\n",
    "\n",
    "        # *****END CODE\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        # *****START CODE\n",
    "\n",
    "        # *****END CODE\n",
    "\n",
    "    def softmax(self, x):\n",
    "        # Softmax activation function for the output layer\n",
    "        # *****START CODE\n",
    "\n",
    "\n",
    "\n",
    "        # *****END CODE\n",
    "\n",
    "    def forward(self, X):\n",
    "        # *****START CODE\n",
    "\n",
    "\n",
    "\n",
    "        # *****END CODE\n",
    "\n",
    "    def backward(self, X, y, learning_rate):\n",
    "        # *****START CODE\n",
    "\n",
    "\n",
    "\n",
    "        # *****END CODE\n",
    "\n",
    "    def train(self, X, y, epochs, learning_rate):\n",
    "        # *****START CODE\n",
    "\n",
    "\n",
    "\n",
    "        # *****END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model has been defined, let's initialize it, and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 64\n",
    "hidden_dim = 32\n",
    "output_dim = 10\n",
    "model = SimpleMLP(input_dim, hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(train_x, train_y, 2000, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check the performance on the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_prediction(model, x, y):\n",
    "    predictions = model.forward(x).argmax(axis=-1)\n",
    "    top1_accuracy = (y == predictions).mean()\n",
    "    return top1_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy on the training set {evaluate_prediction(model, train_x, train_y):.4f}.')\n",
    "print(f'Accuracy on the validation set {evaluate_prediction(model, val_x, val_y):.4f}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "fd75362e27048f1ead3b65beb4812b1da3d387150557ce53b099093c32022a5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
