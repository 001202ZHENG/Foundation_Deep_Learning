{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNx-DlWhUojN"
   },
   "source": [
    "# Deep Learning Course: Lab Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfmNFn5pUojN"
   },
   "source": [
    "In this lab exercise you will become familiar with the PyTorch library in order to solve deep learning problems. The goals of this assignments are as follows:\n",
    "\n",
    "- familiarize with PyTorch Tensors\n",
    "\n",
    "- understand how feedforward backpropagation works in neural networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKVnjTPMvC_S"
   },
   "source": [
    "First time using a Jupyter Notebook or Google Colab? Check [this Jupyter Notebook 101](https://www.kaggle.com/code/jhoward/jupyter-notebook-101).\n",
    "During all the courses you will be asked more than just applying the lectures: check the documentation, ask what you want to do on your favorite search engine or ask the TAs. The Deep Learning community is really open to new practionners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4j7Gf5LUojO"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efuanY_UUojO"
   },
   "source": [
    "For this exercise the only thing you need is this notebook.\n",
    "\n",
    "You may use your own Python environment or use Google Colab. If you choose to use Google Colab, you can upload this notebook to your Google Drive and open it with Google Colab (right click on the file and choose \"Open with\" -> \"Google Colab\").\n",
    "\n",
    "To set up the environment on your own machine, you need to install PyTorch. You can find the instructions [here](https://pytorch.org/get-started/locally/).\n",
    "\n",
    "For more information about Python environment, you may take a look at [conda](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html) or [virtualenv](https://virtualenv.pypa.io/en/latest/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZumXYTYUojO"
   },
   "source": [
    "# Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-6WBdq3UojO"
   },
   "source": [
    "Apart from the Questions, there are instruction comments throughout the notebook as well as comments inside the code cells beginning with two hashtags (##). In addition, there are #**START CODE  /  #**END CODE comments indicating the start and end of your code sections. Pay attention not to delete these comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_YQ2FNeqUojP"
   },
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2LYwZGKUojP"
   },
   "source": [
    "# Q1 - PyTorch Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOus3aXGUojP"
   },
   "source": [
    "a) Get familiar with PyTorch Tensors and construct different types of them. You may take a look at the [documentation](https://pytorch.org/docs/stable/tensors.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "g95S8R61UojP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "##Construct a 5x3 matrix, uninitialized\n",
    "# *****START CODE\n",
    "x = torch.ones((5,3))\n",
    "# *****END CODE\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "MkCzwPzxUojQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.1149, -0.3527,  1.4650],\n",
      "        [ 0.6013, -0.2879, -0.0568],\n",
      "        [ 0.7903, -1.3198, -0.7169],\n",
      "        [-1.1922,  1.3885, -1.4138],\n",
      "        [ 0.7308, -0.9422,  0.2039]])\n"
     ]
    }
   ],
   "source": [
    "##Construct a randomly initialized matrix from a normal distribution\n",
    "# *****START CODE\n",
    "\n",
    "x = torch.randn((5,3))\n",
    "# *****END CODE\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pe99pXJHUojQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "##Construct a matrix filled with zeros and of dtype int64\n",
    "# *****START CODE\n",
    "\n",
    "x = torch.zeros((5,3),dtype= torch.int64)\n",
    "# *****END CODE\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "R43-VxQ6UojQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "##Construct a tensor directly from data\n",
    "# *****START CODE\n",
    "data =  [1,2],[3,4]\n",
    "x = torch.tensor(data)\n",
    "# *****END CODE\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UORgptIYUojR"
   },
   "source": [
    "#Q2 Backpropagation from scratch\n",
    "\n",
    "- Create random input and output PyTorch tensors and train a simple network from scratch.\n",
    "\n",
    "  Warning: You should NOT use any forward/backward commands from PyTorch             library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "QtrAESF-UojR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x torch.Size([64, 1000])\n",
      "y torch.Size([64, 10])\n",
      "y\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "## N is batch size; 批量处理大小，表示一次传递给神经网络的样本数量\n",
    "# D_in is input dimension 输入的特征数量\n",
    "## H is hidden dimenion; \n",
    "# D_out is output dimension\n",
    "\n",
    "torch.manual_seed(10086)\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "## Create random input (x) and output (y) data\n",
    "# *****START CODE\n",
    "x = torch.randn(N,D_in)\n",
    "y = torch.randn(N,D_out)\n",
    "print('x', x.shape)\n",
    "print('y', y.shape)\n",
    "# *****END CODE\n",
    "print('y')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "zofILGJ3VCBz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1 torch.Size([1000, 100])\n",
      "w2 torch.Size([100, 10])\n"
     ]
    }
   ],
   "source": [
    "##Randomly initialize weights from a normal distribution, skipping bias\n",
    "##Hint: You need 2 weight tensors; one for the raw input tensor (x) and one for the hidden dimension\n",
    "# *****START CODE\n",
    "w1 = torch.randn(D_in, H)\n",
    "w2 = torch.randn(H,D_out )\n",
    "# *****END CODE\n",
    "print('w1', w1.shape)\n",
    "print('w2', w2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "9sQA5kvXVLge"
   },
   "outputs": [],
   "source": [
    "##define the learning rate\n",
    "learning_rate = 1e-6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9xgDIJGcVfnb"
   },
   "source": [
    "First, implement the forward pass. Try to compute the predicted y_pred value. You can take a look [here](https://pytorch.org/docs/stable/nn.functional.html#non-linear-activations-weighted-sum-nonlinearity) for more information about activation functions in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "OlQ0tMGsVQrz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x torch.Size([64, 1000])\n",
      "h torch.Size([64, 100])\n"
     ]
    }
   ],
   "source": [
    "## Calculate the output of the hidden dimension\n",
    "## Hint: make use of torch.matmul()\n",
    "# *****START CODE\n",
    "import torch.nn.functional as F\n",
    "# x : N x D_in\n",
    "# w1 : D_in x H\n",
    "# h : N x H\n",
    "#h = torch.matmul(x, w1)\n",
    "h = x @ w1                # output of the hidden dimension \n",
    "# *****END CODE\n",
    "print('x', x.shape)\n",
    "print('h', h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "8NbtF0QtWA25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 100])\n"
     ]
    }
   ],
   "source": [
    "## Pass the output of the hidden dimension to the ReLU activation function\n",
    "# *****START CODE\n",
    "h_relu =  F.relu(h)          # output of the ReLU function\n",
    "# *****END CODE  \n",
    "print(h_relu.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "xAMaSc7JWA-E"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "## Calculate the final output of the network\n",
    "# *****START CODE\n",
    "y_pred = torch.matmul(h_relu, w2 )       # final output of the network\n",
    "# *****END CODE\n",
    "print('y_pred', y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xHdnKovWvIk"
   },
   "source": [
    "Calculate the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "usvfBgnWVQ4T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(33264.1211)\n"
     ]
    }
   ],
   "source": [
    "## Compute loss\n",
    "loss = ((y_pred - y) ** 2).mean()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QklmgLMnW1k_"
   },
   "source": [
    "Now, implement the backward pass.\n",
    "You need to minimize the loss with respect to each weight using the chain rule of differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "rOlsl-15W35z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_loss_d_y_pred torch.Size([64, 10])\n",
      "d_y_pred_d_w2 torch.Size([64, 100])\n",
      "w2 torch.Size([100, 10])\n",
      "d_loss_d_w2 torch.Size([100, 10])\n"
     ]
    }
   ],
   "source": [
    "## Compute the gradient of w2 with respect to the loss\n",
    "# *****START CODE\n",
    "d_loss_dy_predict = 2*(y_pred-y)\n",
    "\n",
    "dy_predict_dw2 = h_relu\n",
    "\n",
    "print('d_loss_d_y_pred', d_loss_dy_predict.shape)\n",
    "print('d_y_pred_d_w2', dy_predict_dw2.shape)\n",
    "\n",
    "d_loss_d_w2 = dy_predict_dw2.T @ d_loss_dy_predict\n",
    "\n",
    "print('w2', w2.shape)\n",
    "print('d_loss_d_w2', d_loss_d_w2.shape)\n",
    "\n",
    "# *****END CODE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "PVWLJdcwXF_C"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_loss_d_y_pred torch.Size([64, 10])\n",
      "d_y_pred_d_h torch.Size([100, 10])\n",
      "d_h_d_w1 torch.Size([64, 1000])\n",
      "d_loss_d_w1 first step torch.Size([64, 100])\n",
      "d_loss_d_w1 torch.Size([1000, 100])\n",
      "w1 torch.Size([1000, 100])\n"
     ]
    }
   ],
   "source": [
    "## Compute the gradient of w1 with respect to the loss (consider the derivative of ReLU equal to 1)\n",
    "# *****START CODE\n",
    "\n",
    "d_loss_d_y_pred = 2.0 * (y_pred - y)\n",
    "d_y_pred_d_h = w2\n",
    "d_h_d_w1 = x\n",
    "\n",
    "print('d_loss_d_y_pred', d_loss_d_y_pred.shape)\n",
    "print('d_y_pred_d_h', d_y_pred_d_h.shape)\n",
    "print('d_h_d_w1', d_h_d_w1.shape)\n",
    "\n",
    "d_loss_d_w1 = d_loss_d_y_pred @ d_y_pred_d_h.t()\n",
    "print('d_loss_d_w1 first step', d_loss_d_w1.shape)\n",
    "d_loss_d_w1 = d_h_d_w1.t() @ d_loss_d_w1\n",
    "print('d_loss_d_w1', d_loss_d_w1.shape)\n",
    "print('w1', w1.shape)\n",
    "# *****END CODE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "cdZx29tlXNUx"
   },
   "outputs": [],
   "source": [
    "## Update weights\n",
    "# *****START CODE\n",
    "w1 = w1 - learning_rate * d_loss_d_w1\n",
    "w2 = w2 - learning_rate * d_loss_d_w2\n",
    "# *****END CODE\n",
    "# *****END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2DWDA8CZXU5p"
   },
   "source": [
    "Repeat the above process for a number of epochs and notice how the value of the loss changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "jWbfq-36XkRN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss 23794.025390625\n",
      "Epoch 1 loss 19929.552734375\n",
      "Epoch 2 loss 19164.29296875\n",
      "Epoch 3 loss 20235.326171875\n",
      "Epoch 4 loss 22296.8203125\n",
      "Epoch 5 loss 24155.28515625\n",
      "Epoch 6 loss 24867.455078125\n",
      "Epoch 7 loss 23369.33203125\n",
      "Epoch 8 loss 19976.48046875\n",
      "Epoch 9 loss 15346.3408203125\n",
      "Epoch 10 loss 10900.568359375\n",
      "Epoch 11 loss 7265.74072265625\n",
      "Epoch 12 loss 4724.6865234375\n",
      "Epoch 13 loss 3056.13720703125\n",
      "Epoch 14 loss 2020.9068603515625\n",
      "Epoch 15 loss 1381.6683349609375\n",
      "Epoch 16 loss 987.9893798828125\n",
      "Epoch 17 loss 738.98291015625\n",
      "Epoch 18 loss 576.8760375976562\n",
      "Epoch 19 loss 466.63946533203125\n",
      "Epoch 20 loss 388.38018798828125\n",
      "Epoch 21 loss 330.255615234375\n",
      "Epoch 22 loss 285.3699645996094\n",
      "Epoch 23 loss 249.4702911376953\n",
      "Epoch 24 loss 219.98355102539062\n",
      "Epoch 25 loss 195.23509216308594\n",
      "Epoch 26 loss 174.0675506591797\n",
      "Epoch 27 loss 155.8298797607422\n",
      "Epoch 28 loss 139.9313201904297\n",
      "Epoch 29 loss 125.99918365478516\n",
      "Epoch 30 loss 113.6941146850586\n",
      "Epoch 31 loss 102.8029556274414\n",
      "Epoch 32 loss 93.11150360107422\n",
      "Epoch 33 loss 84.45902252197266\n",
      "Epoch 34 loss 76.72779083251953\n",
      "Epoch 35 loss 69.80577087402344\n",
      "Epoch 36 loss 63.59442138671875\n",
      "Epoch 37 loss 57.99640655517578\n",
      "Epoch 38 loss 52.94514083862305\n",
      "Epoch 39 loss 48.3857307434082\n",
      "Epoch 40 loss 44.26478958129883\n",
      "Epoch 41 loss 40.54297637939453\n",
      "Epoch 42 loss 37.16755676269531\n",
      "Epoch 43 loss 34.10399627685547\n",
      "Epoch 44 loss 31.319713592529297\n",
      "Epoch 45 loss 28.78745460510254\n",
      "Epoch 46 loss 26.479930877685547\n",
      "Epoch 47 loss 24.37671661376953\n",
      "Epoch 48 loss 22.46364402770996\n",
      "Epoch 49 loss 20.718494415283203\n",
      "Epoch 50 loss 19.122455596923828\n",
      "Epoch 51 loss 17.661413192749023\n",
      "Epoch 52 loss 16.322742462158203\n",
      "Epoch 53 loss 15.095046997070312\n",
      "Epoch 54 loss 13.967666625976562\n",
      "Epoch 55 loss 12.9316987991333\n",
      "Epoch 56 loss 11.979400634765625\n",
      "Epoch 57 loss 11.103296279907227\n",
      "Epoch 58 loss 10.296581268310547\n",
      "Epoch 59 loss 9.55223274230957\n",
      "Epoch 60 loss 8.865924835205078\n",
      "Epoch 61 loss 8.232810974121094\n",
      "Epoch 62 loss 7.6479034423828125\n",
      "Epoch 63 loss 7.107635498046875\n",
      "Epoch 64 loss 6.608461856842041\n",
      "Epoch 65 loss 6.146979331970215\n",
      "Epoch 66 loss 5.720101356506348\n",
      "Epoch 67 loss 5.324761867523193\n",
      "Epoch 68 loss 4.958484649658203\n",
      "Epoch 69 loss 4.620354652404785\n",
      "Epoch 70 loss 4.306954383850098\n",
      "Epoch 71 loss 4.016290664672852\n",
      "Epoch 72 loss 3.746582508087158\n",
      "Epoch 73 loss 3.4962170124053955\n",
      "Epoch 74 loss 3.263698101043701\n",
      "Epoch 75 loss 3.047243356704712\n",
      "Epoch 76 loss 2.8459842205047607\n",
      "Epoch 77 loss 2.6588492393493652\n",
      "Epoch 78 loss 2.484771251678467\n",
      "Epoch 79 loss 2.322774648666382\n",
      "Epoch 80 loss 2.171983242034912\n",
      "Epoch 81 loss 2.031555414199829\n",
      "Epoch 82 loss 1.9007537364959717\n",
      "Epoch 83 loss 1.7788562774658203\n",
      "Epoch 84 loss 1.6652133464813232\n",
      "Epoch 85 loss 1.559251070022583\n",
      "Epoch 86 loss 1.4603919982910156\n",
      "Epoch 87 loss 1.368146300315857\n",
      "Epoch 88 loss 1.2820518016815186\n",
      "Epoch 89 loss 1.2016936540603638\n",
      "Epoch 90 loss 1.1266196966171265\n",
      "Epoch 91 loss 1.0562711954116821\n",
      "Epoch 92 loss 0.990531325340271\n",
      "Epoch 93 loss 0.9290978312492371\n",
      "Epoch 94 loss 0.8716721534729004\n",
      "Epoch 95 loss 0.8179677724838257\n",
      "Epoch 96 loss 0.7677469253540039\n",
      "Epoch 97 loss 0.7207610011100769\n",
      "Epoch 98 loss 0.6767967939376831\n",
      "Epoch 99 loss 0.6356343626976013\n"
     ]
    }
   ],
   "source": [
    "##specify the number of epochs\n",
    "# *****START CODE\n",
    "epochs = 100\n",
    "# *****END CODE\n",
    "\n",
    "for t in range(epochs):\n",
    "  # *****START CODE\n",
    "\n",
    "  # forward pass\n",
    "  h = x @ w1\n",
    "  h_relu = F.relu(h)\n",
    "  y_pred = h_relu @ w2\n",
    "  \n",
    "  # compute the loss\n",
    "  loss = ((y_pred - y) ** 2).mean()\n",
    "\n",
    "  # compute the gradient wrt w2\n",
    "  d_loss_d_y_pred = 2.0 * (y_pred - y)\n",
    "  d_y_pred_d_w2 = h_relu\n",
    "  d_loss_d_w2 = d_y_pred_d_w2.T @ d_loss_d_y_pred\n",
    "\n",
    "  # compute the gradient wrt w1\n",
    "  d_loss_d_y_pred = 2.0 * (y_pred - y)\n",
    "  d_y_pred_d_h = w2\n",
    "  d_h_d_w1 = x\n",
    "  d_loss_d_w1 = d_loss_d_y_pred @ d_y_pred_d_h.t()\n",
    "  d_loss_d_w1 = d_h_d_w1.t() @ d_loss_d_w1\n",
    "\n",
    "  # update the weights\n",
    "  w1 = w1 - learning_rate * d_loss_d_w1\n",
    "  w2 = w2 - learning_rate * d_loss_d_w2\n",
    "\n",
    "  print('Epoch', t, 'loss', loss.item())\n",
    "\n",
    "  # *****END CODE\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "fd75362e27048f1ead3b65beb4812b1da3d387150557ce53b099093c32022a5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
